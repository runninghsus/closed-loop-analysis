{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "from tqdm import tqdm\n",
    "from tqdm import notebook\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adp_filt(currdf: object, pose):\n",
    "    lIndex = []\n",
    "    xIndex = []\n",
    "    yIndex = []\n",
    "    currdf = np.array(currdf[1:])\n",
    "    for header in pose:\n",
    "        if currdf[0][header + 1] == \"likelihood\":\n",
    "            lIndex.append(header)\n",
    "        elif currdf[0][header + 1] == \"x\":\n",
    "            xIndex.append(header)\n",
    "        elif currdf[0][header + 1] == \"y\":\n",
    "            yIndex.append(header)\n",
    "    curr_df1 = currdf[:, 1:]\n",
    "    datax = curr_df1[1:, np.array(xIndex)]\n",
    "    datay = curr_df1[1:, np.array(yIndex)]\n",
    "    data_lh = curr_df1[1:, np.array(lIndex)]\n",
    "    currdf_filt = np.zeros((datax.shape[0], (datax.shape[1]) * 2))\n",
    "    perc_rect = []\n",
    "    for i in range(data_lh.shape[1]):\n",
    "        perc_rect.append(0)\n",
    "    for x in tqdm(range(data_lh.shape[1])):\n",
    "        a, b = np.histogram(data_lh[1:, x].astype(np.float))\n",
    "        rise_a = np.where(np.diff(a) >= 0)\n",
    "        if rise_a[0][0] > 1:\n",
    "            llh = b[rise_a[0][0]]\n",
    "        else:\n",
    "            llh = b[rise_a[0][1]]\n",
    "        #llh=0\n",
    "        data_lh_float = data_lh[:, x].astype(np.float)\n",
    "        perc_rect[x] = np.sum(data_lh_float < llh) / data_lh.shape[0]\n",
    "        currdf_filt[0, (2 * x):(2 * x + 2)] = np.hstack([datax[0, x], datay[0, x]])\n",
    "        for i in range(1, data_lh.shape[0]):\n",
    "            if data_lh_float[i] < llh:\n",
    "                currdf_filt[i, (2 * x):(2 * x + 2)] = currdf_filt[i - 1, (2 * x):(2 * x + 2)]\n",
    "            else:\n",
    "                currdf_filt[i, (2 * x):(2 * x + 2)] = np.hstack([datax[i, x], datay[i, x]])\n",
    "    currdf_filt = np.array(currdf_filt)\n",
    "    currdf_filt = currdf_filt.astype(np.float)\n",
    "    return currdf_filt, perc_rect\n",
    "\n",
    "def boxcar_center(a, n):\n",
    "    a1 = pd.Series(a)\n",
    "    moving_avg = np.array(a1.rolling(window=n, min_periods=1, center=True).mean())\n",
    "\n",
    "    return moving_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## load deeplabcut csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tk root\n",
    "root = Tk()\n",
    "\n",
    "# Hide the main window\n",
    "root.withdraw()\n",
    "root.call('wm', 'attributes', '.', '-topmost', True)\n",
    "infiles = filedialog.askopenfilename(multiple=True, title='load deeplabcut csv')\n",
    "    \n",
    "%gui tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe for all files\n",
    "df_all = []\n",
    "for f in range(len(infiles)):\n",
    "    df_all.append(pd.read_csv(infiles[f], low_memory=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter based on likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_input_data = []\n",
    "for f in range(len(df_all)):\n",
    "    csv_array_filtered, perc_filtered = adp_filt(df_all[f], np.arange(18))\n",
    "    # making it into a list, feature extraction code works with multiple files in a list\n",
    "    processed_input_data.append(csv_array_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yttri-Lab\\AppData\\Local\\Temp\\ipykernel_74404\\1600903588.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  window = np.int(np.round(0.05 / (1 / framerate)) * 2 - 1)\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "framerate = 30\n",
    "window = np.int(np.round(0.05 / (1 / framerate)) * 2 - 1)\n",
    "f = []\n",
    "for n in tqdm(range(len(processed_input_data))):\n",
    "    data_n_len = len(processed_input_data[n])\n",
    "    dxy_list = []\n",
    "    disp_list = []\n",
    "    for r in range(data_n_len):\n",
    "        if r < data_n_len - 1:\n",
    "            disp = []\n",
    "            for c in range(0, processed_input_data[n].shape[1], 2):\n",
    "                disp.append(\n",
    "                    np.linalg.norm(processed_input_data[n][r + 1, c:c + 2] -\n",
    "                                   processed_input_data[n][r, c:c + 2]))\n",
    "            disp_list.append(disp)\n",
    "        dxy = []\n",
    "        for i, j in itertools.combinations(range(0, processed_input_data[n].shape[1], 2), 2):\n",
    "            dxy.append(processed_input_data[n][r, i:i + 2] -\n",
    "                       processed_input_data[n][r, j:j + 2])\n",
    "        dxy_list.append(dxy)\n",
    "    disp_r = np.array(disp_list)\n",
    "    dxy_r = np.array(dxy_list)\n",
    "    disp_boxcar = []\n",
    "    dxy_eu = np.zeros([data_n_len, dxy_r.shape[1]])\n",
    "    ang = np.zeros([data_n_len - 1, dxy_r.shape[1]])\n",
    "    dxy_boxcar = []\n",
    "    ang_boxcar = []\n",
    "    for l in range(disp_r.shape[1]):\n",
    "        disp_boxcar.append(boxcar_center(disp_r[:, l], window))\n",
    "        # disp_boxcar.append(disp_r[:, l])\n",
    "    for k in range(dxy_r.shape[1]):\n",
    "        for kk in range(data_n_len):\n",
    "            dxy_eu[kk, k] = np.linalg.norm(dxy_r[kk, k, :])\n",
    "            if kk < data_n_len - 1:\n",
    "                b_3d = np.hstack([dxy_r[kk + 1, k, :], 0])\n",
    "                a_3d = np.hstack([dxy_r[kk, k, :], 0])\n",
    "                c = np.cross(b_3d, a_3d)\n",
    "                ang[kk, k] = np.dot(np.dot(np.sign(c[2]), 180) / np.pi,\n",
    "                                    math.atan2(np.linalg.norm(c),\n",
    "                                               np.dot(dxy_r[kk, k, :], dxy_r[kk + 1, k, :])))\n",
    "        \n",
    "        dxy_boxcar.append(boxcar_center(dxy_eu[:, k], window))\n",
    "        ang_boxcar.append(boxcar_center(ang[:, k], window))\n",
    "        # dxy_boxcar.append(dxy_eu[:, k])\n",
    "        # ang_boxcar.append(ang[:, k])\n",
    "    disp_feat = np.array(disp_boxcar)\n",
    "    dxy_feat = np.array(dxy_boxcar)\n",
    "    ang_feat = np.array(ang_boxcar)\n",
    "    f.append(np.vstack((dxy_feat[:, 1:], ang_feat, disp_feat)))\n",
    "    \n",
    "for m in range(0, len(f)):\n",
    "    f_integrated = np.zeros(len(processed_input_data[m]))\n",
    "    for k in range(round(framerate / 10), len(f[m][0]), round(framerate / 10)):\n",
    "        if k > round(framerate / 10):\n",
    "            f_integrated = np.concatenate(\n",
    "                (f_integrated.reshape(f_integrated.shape[0], f_integrated.shape[1]),\n",
    "                np.hstack((np.mean((f[m][0:dxy_feat.shape[0],\n",
    "                                         range(k - round(framerate / 10), k)]), axis=1),\n",
    "                           np.sum((f[m][dxy_feat.shape[0]:f[m].shape[0],\n",
    "                                        range(k - round(framerate / 10), k)]), axis=1)\n",
    "                          )).reshape(len(f[0]), 1)), axis=1\n",
    "                )\n",
    "        else:\n",
    "            f_integrated = np.hstack(\n",
    "                (np.mean((f[m][0:dxy_feat.shape[0], range(k - round(framerate / 10), k)]), axis=1),\n",
    "                 np.sum((f[m][dxy_feat.shape[0]:f[m].shape[0],\n",
    "                              range(k - round(framerate / 10), k)]), axis=1))).reshape(len(f[0]), 1)\n",
    "            \n",
    "    features.append(f_integrated)\n",
    "    \n",
    "    #if m > 0:\n",
    "    #     features = np.concatenate((features, f_integrated), axis=1)\n",
    "    # else:\n",
    "    #     features = f_integrated\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If on Yttri Lab: Uncomment this one! \n",
    "with open(r'Z:\\hsu\\output\\ag_n3iter13_kw_ref_n6_randomforest.sav', 'rb') as fr:\n",
    "\n",
    "#if using Z drive: uncomment this one! \n",
    "#with open(r'Z:\\temp\\iterX_usable_new.sav', 'rb') as fr:\n",
    "    \n",
    "# If on local: Uncomment this one! \n",
    "#with open('/Volumes/yttri/temp/iterX_usable_new.sav', 'rb') as fr: \n",
    "    random_forest_sav = joblib.load(fr)\n",
    "    clf = random_forest_sav[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.n_classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original code written by Kaity \n",
    "#If on jimi: Uncomment this one! \n",
    "#with open(r'C:\\Users\\Yttri-Lab\\Desktop\\D on Server (NoMachine)\\DeepLabStream\\iterX_usable_new.sav','rb') as fr:\n",
    "\n",
    "#If on local: Uncomment this one! \n",
    "#with open('/Volumes/yttri/temp/iterX_usable_new.sav', 'rb') as fr: \n",
    "  #  random_forest_sav = joblib.load(fr)\n",
    "   # clf = random_forest_sav[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [clf.predict(features[f].T) for f in tqdm(range(len(features)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction, infile in zip(predictions, infiles):\n",
    "    df_pred = pd.DataFrame(prediction)\n",
    "    filename_parts = infile.rpartition('DLC')\n",
    "    if len(filename_parts) == 3:\n",
    "        filename = ''.join([filename_parts[0], '_posthoc-predictions.csv'])\n",
    "        df_pred.to_csv(filename, index=False)\n",
    "    else:\n",
    "        # Handle the case where 'DLC' is not found in the filename\n",
    "        # You may want to specify a different filename or handle this case differently.\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for prediction in predictions:\n",
    "    df_pred = pd.DataFrame(prediction)\n",
    "    df_pred.to_csv(str.join('', (infiles[f].rpartition('DLC')[0], '_posthoc-predictions.csv')), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class 1 and 2 are turns\n",
    "class_of_interest=2\n",
    "jitter = 0.5\n",
    "rows = len(predictions)\n",
    "cols = 1\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(16, 11))\n",
    "for row in range(rows):\n",
    "    if rows == 1:\n",
    "        ax.hlines(class_of_interest-jitter, 0, len(predictions[row]), 'r', alpha=0.7)\n",
    "        ax.hlines(class_of_interest+jitter, 0, len(predictions[row]), 'r', alpha=0.7)\n",
    "        ax.vlines(0-jitter, class_of_interest-jitter, class_of_interest+jitter, 'r', alpha=0.7)\n",
    "        ax.vlines(len(predictions[row])+jitter, class_of_interest-jitter, class_of_interest+jitter, 'r', alpha=0.7)\n",
    "        ax.scatter(np.arange(len(predictions[row])), predictions[row], s=0.5, color='k', alpha=0.3)\n",
    "        ax.set_xticks(np.arange(0, len(predictions[row]), 6000))\n",
    "        ax.set_xticklabels(np.arange(0, len(predictions[row])/10, 600))\n",
    "        ax.set_ylabel('behavior ID')\n",
    "        ax.set_xlabel('time (s)')\n",
    "    else:\n",
    "        try:\n",
    "            ax[row].set_title(f'vid{infiles[row].rpartition(\"DLC\")[0].rpartition(\"/\")[2].rpartition(\" 0-\")[2]}')\n",
    "            ax[row].hlines(class_of_interest-jitter, 0, len(predictions[row]), 'r', alpha=0.7)\n",
    "            ax[row].hlines(class_of_interest+jitter, 0, len(predictions[row]), 'r', alpha=0.7)\n",
    "            ax[row].vlines(0-jitter, class_of_interest-jitter, class_of_interest+jitter, 'r', alpha=0.7)\n",
    "            ax[row].vlines(len(predictions[row])+jitter, class_of_interest-jitter, class_of_interest+jitter, 'r', alpha=0.7)\n",
    "            ax[row].scatter(np.arange(len(predictions[row])), predictions[row], s=0.5, color='k', alpha=0.3)\n",
    "            ax[row].set_xticks(np.arange(0, len(predictions[row]), 6000))\n",
    "            ax[row].set_xticklabels(np.arange(0, len(predictions[row])/10, 600))\n",
    "            ax[row].set_ylabel('behavior ID')\n",
    "            ax[row].set_xlabel('time (s)')\n",
    "        except:\n",
    "            pass\n",
    "fig.savefig('./test.svg', dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(len(infiles)):\n",
    "    fileoutname = str.join('', (infiles[f].rpartition('DLC')[0], '_posthoc-predictions.npy'))\n",
    "    np.save(fileoutname, predictions[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
