{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bout, Stim, and Duration Post Hoc Analysis(CSV for Database) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tqdm import tqdm\n",
    "from tqdm import notebook\n",
    "import itertools\n",
    "import math\n",
    "import seaborn as sns\n",
    "import statistics \n",
    "from collections import OrderedDict\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Z:/KayCei/HDC/AH1-KO/120423-LIGHTS OFF/predict_video_exp_16.npy',)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Tk root\n",
    "root = Tk()\n",
    "# Hide the main window\n",
    "root.withdraw()\n",
    "root.call('wm', 'attributes', '.', '-topmost', True)\n",
    "infiles = filedialog.askopenfilename(multiple=True, title='load posthoc-predictions.npy')\n",
    "\n",
    "%gui tk\n",
    "\n",
    "infiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the user selected any files\n",
    "if infiles:\n",
    "    # Assuming infiles is a list of selected file paths, let's use the first one\n",
    "    selected_file = infiles[0]\n",
    "\n",
    "    # Extract the directory path from the selected file's full path\n",
    "    directory_path = os.path.dirname(selected_file)\n",
    "    \n",
    "#print(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random_List = [] #creates a array known as random list \n",
    "for f in range(len(infiles)):\n",
    "    arr = np.load(infiles[f])\n",
    "    Random_List.extend(arr) \n",
    "#print(Random_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_labels = {\n",
    "    0.0: \"insignificant\",\n",
    "    1.0: \"insignificant\",\n",
    "    2.0: \"insignificant\",\n",
    "    3.0: \"insignificant\",\n",
    "    4.0: \"Rear\",\n",
    "    5.0: \"insignificant\",\n",
    "    6.0: \"insignificant\",\n",
    "    7.0: \"Investigate Type 1\",\n",
    "    8.0: \"Investigate Type 2\", \n",
    "    9.0: \"Contra-Itch\",\n",
    "    10.0: \"Investigate Type 3\",\n",
    "    11.0: \"insignificant\",\n",
    "    12.0: \"insignificant\",\n",
    "    13.0: \"insignificant\",\n",
    "    14.0: \"Contra-Body Groom\",\n",
    "    15.0: \"Face Groom Type 1\",\n",
    "    16.0: \"Dive/Scrunch\",\n",
    "    17.0: \"Head Groom\",\n",
    "    18.0: \"Ipsi-Orient\",\n",
    "    19.0: \"insignificant\",\n",
    "    20.0: \"Face Groom Type 2\",\n",
    "    21.0: \"Ipsi-Body Groom\",\n",
    "    22.0: \"Ipsi-Itch Type 1\",\n",
    "    23.0: \"insignificant\",\n",
    "    24.0: \"insignificant\",\n",
    "    25.0: \"Paw-Groom\",\n",
    "    26.0: \"Locomotion\",\n",
    "    27.0: \"insignificant\",\n",
    "    28.0: \"Contra-Orient\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 26, Count: 13509\n",
      "Label: 28, Count: 2211\n",
      "Label: 18, Count: 2810\n",
      "Label: 8, Count: 12643\n",
      "Label: 27, Count: 678\n",
      "Label: 3, Count: 940\n",
      "Label: 15, Count: 342\n",
      "Label: 7, Count: 1215\n",
      "Label: 5, Count: 841\n",
      "Label: 6, Count: 621\n",
      "Label: 9, Count: 1136\n",
      "Label: 16, Count: 341\n",
      "Label: 22, Count: 237\n",
      "Label: 23, Count: 303\n",
      "Label: 10, Count: 591\n",
      "Label: 21, Count: 203\n",
      "Label: 25, Count: 23\n",
      "Label: 24, Count: 64\n",
      "Label: 17, Count: 74\n",
      "Label: 14, Count: 224\n",
      "Label: 4, Count: 55\n",
      "Label: 1, Count: 54\n",
      "Label: 19, Count: 116\n",
      "Label: 11, Count: 30\n",
      "Label: 13, Count: 33\n",
      "Label: 20, Count: 4\n",
      "Label: 12, Count: 19\n",
      "Label: 0, Count: 13\n",
      "Label: 2, Count: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Assuming Random_List contains behavior labels as values\n",
    "label_counts = Counter(Random_List)\n",
    "\n",
    "# Print the length of each behavior label\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label: {label}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Time of Video in deciseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This video is 39331 deciseconds, which is 65.55166666666666 minutes, which is 117993 frames\n"
     ]
    }
   ],
   "source": [
    "duration_in_deciseconds = len(Random_List)\n",
    "duration_in_minutes = duration_in_deciseconds / 600\n",
    "duration_in_frames = duration_in_deciseconds * 3\n",
    "\n",
    "print('This video is', duration_in_deciseconds, 'deciseconds, which is', duration_in_minutes, 'minutes, which is', duration_in_frames, 'frames')\n",
    "#print(Random_List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Integration (deciseconds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BehaviorTime = []\n",
    "\n",
    "for index, value in enumerate(Random_List):\n",
    "    BehaviorTime.append((value, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(BehaviorTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensures you only analyze expected length of  Session "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "length_of_session = (int(input(\"How long was the session in minutes?\"))) * 600\n",
    "print(length_of_session)\n",
    "BehaviorTime = BehaviorTime[:(length_of_session+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removes all info after selected end time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This video is ', len(BehaviorTime), ' deciseconds, which is', (len(BehaviorTime)/600), 'minutes' )\n",
    "print()\n",
    "#print(BehaviorTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adds Stim Count and Frame Rate to Array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in BehaviorTime:\n",
    "    behavior = z[0]\n",
    "    time= z[1]\n",
    "behavior_type_and_duration = []\n",
    "current_behavior = BehaviorTime[0][0]\n",
    "current_time = BehaviorTime[0][1]  \n",
    "duration = 0\n",
    "stim_count=0\n",
    "frames = 0\n",
    "\n",
    "for behavior, time in BehaviorTime:\n",
    "    if behavior == current_behavior:\n",
    "        start_time= current_time\n",
    "        duration += 1\n",
    "            \n",
    "    else:\n",
    "        if duration==1:\n",
    "            end_time=start_time+1\n",
    "        else:\n",
    "            end_time= start_time + duration\n",
    "        frames= duration*6\n",
    "        stim_count= round(duration/6)\n",
    "        behavior_type_and_duration.append((current_behavior, duration,frames,stim_count,start_time, end_time))\n",
    "        current_behavior = behavior\n",
    "        current_time = time\n",
    "        start_time= time\n",
    "        duration = 1\n",
    "        end_time= time+1\n",
    "        \n",
    "# Append the final (current_behavior, current_time, count) tuple outside the loop\n",
    "behavior_type_and_duration.append((current_behavior, duration, frames, stim_count, start_time, end_time))\n",
    "\n",
    "# Make a copy of the original behavior_type_and_duration list\n",
    "behaviortdt= list(behavior_type_and_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(behaviortdt)\n",
    "#print(len(behaviortdt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Test Behavior Counts CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store total durations for each behavior label\n",
    "total_durations = {label: 0 for label in behavior_labels.values()}\n",
    "\n",
    "# Iterate through the behavior_type_and_duration list\n",
    "for behavior, duration, _, _, _, _ in behavior_type_and_duration:\n",
    "    # Get the behavior label based on the behavior code\n",
    "    behavior_label = behavior_labels.get(behavior, \"Unknown\")\n",
    "    \n",
    "    # Add the duration to the total for the corresponding behavior label\n",
    "    total_durations[behavior_label] += duration\n",
    "\n",
    "# Print the total duration for each behavior label\n",
    "for behavior_label, total_duration in total_durations.items():\n",
    "    print(f\"{behavior_label}: {total_duration}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length_of_session = (int(input(\"How long was the session in minutes?\"))) * 600 \n",
    "#print(length_of_session) \n",
    "behaviortdt = [tup for tup in behaviortdt if tup[5] <= 39001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This video is ', behaviortdt[-1][-1], ' deciseconds, which is', (behaviortdt[-1][-1]/600), 'minutes' )\n",
    "print()\n",
    "#print(behaviortdt)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for current_behavior, duration, frames, start_time, current_time in behaviortdt:\n",
    "    if duration "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This now have Behavior, duration, frame rate, start Time, and end time in one array grouped into tuples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Bout Count to the Array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enumerated_behaviortdt = []\n",
    "behavior_index = {}  # Dictionary to store behavior and its corresponding index\n",
    "\n",
    "for behavior, duration, frames, stim_count, start_time, end_time in behaviortdt:\n",
    "    if behavior not in behavior_index:\n",
    "        behavior_index[behavior] = 1\n",
    "    else:\n",
    "        behavior_index[behavior] += 1\n",
    "    \n",
    "    bout = behavior_index[behavior]\n",
    "    enumerated_behaviortdt.append((behavior,stim_count,frames, bout, duration,start_time,end_time))\n",
    "\n",
    "print(enumerated_behaviortdt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stim Count Calcultaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BCBDT = []\n",
    "\n",
    "# Calculate durations and update BCBDT\n",
    "#for behavior, duration, frames, bout, start_time, end_time in enumerated_behaviortdt:\n",
    "    #stim_count = round(duration / 3)\n",
    "    #if stim_count < 1:\n",
    "        #stim_count = 0\n",
    "    #if stim_count > 0:\n",
    "        #BCBDT.append((behavior, duration, frames, bout, stim_count, start_time, end_time))\n",
    "\n",
    "# Print the modified BCBDT\n",
    "#print(\"Modified BCBDT:\", BCBDT)\n",
    "\n",
    "# Assign enumerated_behaviortdt to BCBDST\n",
    "BCBDST = enumerated_behaviortdt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_labels = {\n",
    "    0.0: \"insignificant\",\n",
    "    1.0: \"insignificant\",\n",
    "    2.0: \"insignificant\",\n",
    "    3.0: \"insignificant\",\n",
    "    4.0: \"Rear\",\n",
    "    5.0: \"insignificant\",\n",
    "    6.0: \"insignificant\",\n",
    "    7.0: \"Investigate Type 1\",\n",
    "    8.0: \"Investigate Type 2\", \n",
    "    9.0: \"Contra-Itch\",\n",
    "    10.0: \"Investigate Type 3\",\n",
    "    11.0: \"insignificant\",\n",
    "    12.0: \"insignificant\",\n",
    "    13.0: \"insignificant\",\n",
    "    14.0: \"Contra-Body Groom\",\n",
    "    15.0: \"Face Groom Type 1\", \n",
    "    16.0: \"Dive/Scrunch\",\n",
    "    17.0: \"Head Groom\",\n",
    "    18.0: \"Ipsi-Orient\",\n",
    "    19.0: \"insignificant\",\n",
    "    20.0: \"Face Groom Type 2\",\n",
    "    21.0: \"Ipsi-Body Groom\",\n",
    "    22.0: \"Ipsi-Itch Type 1\",\n",
    "    23.0: \"insignificant\",\n",
    "    24.0: \"insignificant\",\n",
    "    25.0: \"Paw-Groom\",\n",
    "    26.0: \"Locomotion\",\n",
    "    27.0: \"insignificant\",\n",
    "    28.0: \"Contra-Orient\",\n",
    "}\n",
    "\n",
    "bdbt_dict = {}  # Dictionary to store lists of behaviors\n",
    "\n",
    "for behavior,stim_count,frames, bout, duration,start_time,end_time in BCBDST:\n",
    "    behavior_label = behavior_labels.get(behavior, \"Unknown Behavior\")\n",
    "    if behavior_label not in bdbt_dict:\n",
    "        bdbt_dict[behavior_label] = []\n",
    "    bdbt_dict[behavior_label].append((behavior, stim_count, frames, bout, duration, start_time,end_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#print(bdbt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_list = bdbt_dict.keys()\n",
    "print(keys_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bdbt_dict is a dictionary that contains the enumerated behaviortdt . It contains keys: \n",
    "    ‘insignificant’, ‘insignificant’, ‘insignificant’, ‘insignificant’, ‘rear’,‘insignificant’, ‘insignificant’, ‘investigate type 1’, ‘investigate type 2’, ‘contra-itch’,‘investigate type 3’, ‘insignificant’, ‘insignificant’, ‘insignificant’, ‘contra-body groom’,‘face groom type 1’, ‘dive/scrunch’, ‘head groom’, ‘ipsi-orient’, ‘insignificant’,‘face groom type 2’, ‘ipsi-body groom’, ‘ipsi-itch type 1’, ‘insignificant’, ‘insignificant’,‘paw groom’, ‘locomotion’, ‘insignificant’, ‘contra-orient’]\n",
    "    \n",
    "The tuples are structed as behavior, stim_count, frame, bout,duration, start time, end time (decieconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in infiles: # if taking from Z Drive \n",
    "    folders = (os.path.dirname(file_path)).split('/')\n",
    "    print(folders)\n",
    "    print(len(folders))\n",
    "    # Check if there are at least 5 elements in the 'folders' list\n",
    "    if len(folders) >= 5:\n",
    "        fol3 = folders[3].split('-')  # Split element at position 3 by hyphen\n",
    "        fol4 = folders[4].split('-')  # Split element at position 4 by hyphen\n",
    "        \n",
    "        print(\"Folder 3:\", fol3)\n",
    "        print(\"Folder 4:\", fol4)\n",
    "    else:\n",
    "        print(\"The folder path does not have enough elements to access positions 3 and 4.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stim Count and Average Duration "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from tabulate import tabulate\n",
    "stimcount_by_key = {}\n",
    "\n",
    "def round_float(value):\n",
    "    return round(value, 3)\n",
    "\n",
    "for key, tuples_list in bdbt_dict.items():\n",
    "    if key=='insignificant':\n",
    "        continue #skip \"insignificant\" key and its data\n",
    "    stimcount = [tuple_item[1] for tuple_item in tuples_list]\n",
    "    sum_of_stimcount = sum(stimcount)\n",
    "    stimcount_by_key[key] = sum_of_stimcount\n",
    "\n",
    "table_data = [(key, round_float(sum_value)) for key, sum_value in stimcount_by_key.items()]\n",
    "headers = [\"Key\", \"Total Number of Stimulations\"]\n",
    "\n",
    "table = tabulate(table_data, headers, tablefmt=\"pretty\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "\n",
    "def round_float(value):\n",
    "    return round(value, 3)\n",
    "\n",
    "average_duration_by_key = {}\n",
    "\n",
    "for key, tuples_list in bdbt_dict.items():\n",
    "    #if key=='insignificant':\n",
    "        #continue #skip \"insignificant\" key and its data\n",
    "    durations = [tuple_item[4] for tuple_item in tuples_list]\n",
    "    average_duration = (sum(durations) / len(durations))/10\n",
    "    average_duration_by_key[key] =average_duration\n",
    "\n",
    "table_data = [(key, avg_duration) for key, avg_duration in average_duration_by_key.items()]\n",
    "headers = [\"Key\", \"Average Duration(seconds)\"]\n",
    "\n",
    "table = tabulate(table_data, headers, tablefmt=\"pretty\")\n",
    "print(table)\n",
    "\n",
    "# Create a dictionary for the CSV data\n",
    "csv_data = {f'{fol3[0]} {fol4[1]} Duration': average_duration_by_key}\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(average_duration_by_key.keys(), average_duration_by_key.values())\n",
    "plt.xlabel(\"Key\")\n",
    "plt.ylabel(\"Average Duration (seconds)\")\n",
    "plt.title(f\"Average Durations by Key {fol3[0]} {fol4[1]}\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "## Check if the user selected any files\n",
    "if infiles:\n",
    "    # Assuming infiles is a list of selected file paths, let's use the first one\n",
    "    selected_file = infiles[0]\n",
    "\n",
    "    # Extract the directory path from the selected file's full path\n",
    "    directory_path = os.path.dirname(selected_file)\n",
    "\n",
    "    # Save the plot in the same directory as the selected file with the specified filename\n",
    "    filename = f'{fol3[0]} {fol4[1]} Duration.png'\n",
    "    save_path = os.path.join(directory_path, filename)\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Plot saved in: {save_path}\")\n",
    "\n",
    "    # Save the CSV data to a CSV file\n",
    "    csv_filename = f'{fol3[0]} {fol4[1]} Duration.csv'\n",
    "        # Construct the full path for saving the CSV in the same directory\n",
    "    csv_save_path = os.path.join(directory_path, csv_filename)\n",
    "\n",
    "    # Save the CSV data to the CSV file\n",
    "    with open(csv_save_path, mode=\"w\", newline=\"\") as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        for dictionary_name, data_dict in csv_data.items():\n",
    "            csv_writer.writerow([dictionary_name])  # Write the dictionary name as a header\n",
    "            csv_writer.writerow([\"Key\", \"Average Duration (seconds)\"])\n",
    "            for key, avg_duration in data_dict.items():\n",
    "                csv_writer.writerow([key, avg_duration])\n",
    "\n",
    "    print(f\"CSV data saved in: {csv_save_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count_by_key = {}\n",
    "\n",
    "for key, tuples_list in bdbt_dict.items():\n",
    "    #if key=='insignificant':\n",
    "        #continue #skip \"insignificant\" key and its data\n",
    "    durations = [tuple_item[4] for tuple_item in tuples_list]\n",
    "    behavior= [tuple_item[0] for tuple_item in tuples_list]\n",
    "    average_count = sum(durations)\n",
    "    \n",
    "    total_count_by_key[key] = average_count\n",
    "    \n",
    "     # Display the result for each key inside the loop\n",
    "    print(f\"Key: {key}, Average Count: {average_count}\")\n",
    "\n",
    "table_data = [(key, avg_count) for key, avg_count in total_count_by_key.items()]\n",
    "headers = [\"Key\", \"Average Count\"]\n",
    "\n",
    "\n",
    "table = tabulate(table_data, headers, tablefmt=\"pretty\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stim_block_stim_count_by_key = {}\n",
    "\n",
    "for key, tuples_list in bdbt_dict.items():\n",
    "    if key=='insignificant':\n",
    "        continue #skip \"insignificant\" key and its data\n",
    "    sum_of_stimcount = 0\n",
    "    for tuple_item in tuples_list:\n",
    "        if tuple_item[5]> 3000 and tuple_item[6] <= 21000:  \n",
    "            sum_of_stimcount += tuple_item[1]\n",
    "    stim_block_stim_count_by_key[key] = sum_of_stimcount\n",
    "\n",
    "table_data = [(key, round_float(sum_value)) for key, sum_value in stim_block_stim_count_by_key.items()]\n",
    "headers = [\"Key\", \"Total Stimulations during 30-Min Stim Block\"]\n",
    "\n",
    "table = tabulate(table_data, headers, tablefmt=\"pretty\")\n",
    "print(table)\n",
    "\n",
    "average_duration_30min_by_key = {}\n",
    "\n",
    "for key, tuples_list in bdbt_dict.items():\n",
    "    if key=='insignificant':\n",
    "        continue #skip \"insignificant\" key and its data\n",
    "    total_duration = 0  # Initialize the total duration to 0\n",
    "    count = 0  # Initialize a count variable to track the number of valid tuples\n",
    "    \n",
    "    for tuple_item in tuples_list:\n",
    "        if tuple_item[5] > 3000 and tuple_item[6] <= 21000:  # Check the conditions\n",
    "            total_duration += tuple_item[4]  # Add the duration to the total\n",
    "            count += 1  # Increment the count\n",
    "            \n",
    "    if count > 0:\n",
    "        average_duration = round(((total_duration / count)/10), 2)  # Calculate average and round it\n",
    "    else:\n",
    "        average_duration = 0  # Set average to 0 if there are no valid tuples\n",
    "    \n",
    "    average_duration_30min_by_key[key] = average_duration\n",
    "\n",
    "# Printing the results in a tabular format\n",
    "table_data = [(key, avg_duration) for key, avg_duration in average_duration_30min_by_key.items()]\n",
    "headers = [\"Key\", \"Average Duration During 30 Min Stim Block (seconds)\"]\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "table = tabulate(table_data, headers, tablefmt=\"pretty\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Number of Bouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bout_by_key = {}\n",
    "\n",
    "for key, tuples_list in bdbt_dict.items():\n",
    "    #if key=='insignificant':\n",
    "        #continue #skip \"insignificant\" key and its data\n",
    "    bouts = [tuple_item[3] for tuple_item in tuples_list]\n",
    "    sum_of_bouts = len(bouts)\n",
    "    bout_by_key[key] = sum_of_bouts\n",
    "\n",
    "table_data = [(key, sum_value) for key, sum_value in bout_by_key.items()]\n",
    "headers = [\"Key\", \"Total Bouts\"]\n",
    "\n",
    "table = tabulate(table_data, headers, tablefmt=\"pretty\")\n",
    "print(table)\n",
    "\n",
    "# Create a dictionary for the CSV data\n",
    "csv_data_bouts = {f'{fol3[0]} {fol4[1]}': bout_by_key}\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bout_by_key.keys(), bout_by_key.values())\n",
    "plt.xlabel(\"Key\")\n",
    "plt.ylabel(\"Total Bouts\")\n",
    "plt.title(f'Total Bouts by Key {fol3[0]} {fol4[1]}')\n",
    "plt.xticks(rotation=45, ha='right')  # Set the x-axis ticks and labels\n",
    "plt.tight_layout()\n",
    "# Check if the user selected any files\n",
    "if infiles:\n",
    "    # Assuming infiles is a list of selected file paths, let's use the first one\n",
    "    selected_file = infiles[0]\n",
    "\n",
    "    # Extract the directory path from the selected file's full path\n",
    "    directory_path = os.path.dirname(selected_file)\n",
    "    # Save the plot as \"(specific animals) Bout.png\"\n",
    "    filename_bout= f'{fol3[0]} {fol4[1]} Bouts.png'\n",
    "    save_path= os.path.join(directory_path,filename_bout)\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    # Save the CSV data to a CSV file\n",
    "    csv_filename_bouts = f'{fol3[0]} {fol4[1]} Bouts.csv'\n",
    "    csv_save_path = os.path.join(directory_path, csv_filename_bouts)\n",
    "    \n",
    "    with open(csv_save_path, mode=\"w\", newline=\"\") as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        for dictionary_name, data_dict in csv_data_bouts.items():\n",
    "            csv_writer.writerow([dictionary_name])  # Write the dictionary name as a header\n",
    "            csv_writer.writerow([\"Key\", \"Total Bouts\"])\n",
    "            for key, sum_value in data_dict.items():\n",
    "                csv_writer.writerow([key, sum_value])\n",
    "    print(f\"CSV data saved in: {csv_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(csv_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in infiles:  # if taking from Z Drive \n",
    "    folders = os.path.dirname(file_path).rsplit('\\\\', 1)\n",
    "    print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file paths for the two CSV files and the output CSV file\n",
    "bout_csv_filename = os.path.join('/'.join(folders), csv_filename_bouts)\n",
    "duration_csv_filename = os.path.join('/'.join(folders), csv_filename )\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "bout_df = pd.read_csv(bout_csv_filename, skiprows=[0])\n",
    "duration_df = pd.read_csv(duration_csv_filename, skiprows=[0])\n",
    "\n",
    "print(\"Columns in bout_df:\", bout_df.columns)\n",
    "print(\"Columns in duration_df:\", duration_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames on the shared key column (e.g., \"Key\")\n",
    "merged_df = bout_df.merge(duration_df, on=\"Key\", how=\"inner\")\n",
    "\n",
    "# Perform the multiplication: Total Bouts * Average Duration (seconds)\n",
    "merged_df[\"Result(seconds)\"] = merged_df[\"Total Bouts\"] * merged_df[\"Average Duration (seconds)\"]\n",
    "\n",
    "# Display the results DataFrame within the Jupyter Notebook\n",
    "display(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum of the \"Result\" column in the DataFrame\n",
    "total_result = (merged_df[\"Result(seconds)\"].sum())/60\n",
    "\n",
    "print(f\"Sum of the 'Result' column: {total_result} minutes\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stim_block_bout_count_by_key = {}\n",
    "\n",
    "for key, tuples_list in bdbt_dict.items():\n",
    "    if key=='insignificant':\n",
    "        continue #skip \"insignificant\" key and its data\n",
    "    bout_count = sum(1 for tuple_item in tuples_list if 3000 < tuple_item[5] and tuple_item[6] <= 21000)\n",
    "    stim_block_bout_count_by_key[key] = bout_count\n",
    "\n",
    "table_data = [(key, count) for key, count in stim_block_bout_count_by_key.items()]\n",
    "headers = [\"Key\", \"30 minute Stim Block Bout Count\"]\n",
    "\n",
    "table = tabulate(table_data, headers, tablefmt=\"pretty\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_dict = {}\n",
    "\n",
    "for key, value_list in bdbt_dict.items():\n",
    "    rounded_values = [\n",
    "        (\n",
    "            round(v[0]),     # Rounding and converting to integer\n",
    "            v[1], v[2], v[3], \n",
    "            round(v[4], 3),\n",
    "            round(v[5], 2),  \n",
    "            round(v[6], 2)   \n",
    "        )\n",
    "        for v in value_list\n",
    "    ]\n",
    "    rounded_dict[key] = rounded_values\n",
    "\n",
    "#print(rounded_dict)\n",
    "\n",
    "bdbt_dict=rounded_dict\n",
    "#print(bdbt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dumps\n",
    "string_bdbt_dict=dumps(bdbt_dict)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(string_bdbt_dict)\n",
    "print(len(string_bdbt_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the start index of the first '{' character\n",
    "start_index = string_bdbt_dict.index('{')\n",
    "\n",
    "# Find the end index of the last '}' character\n",
    "end_index = string_bdbt_dict.rindex('}') + 1\n",
    "\n",
    "# Extract the dictionary portion of the string\n",
    "dict_str = string_bdbt_dict[start_index:end_index]\n",
    "\n",
    "# Find the start indices of key-value pairs using '\":'\n",
    "key_value_end = []\n",
    "for pos, char in enumerate(dict_str):\n",
    "    if dict_str[pos:pos+3] == '\": ':\n",
    "        key_value_end.append(pos)\n",
    "        \n",
    "#print(key_value_end)      \n",
    "\n",
    "# Initialize lists to store extracted keys and values\n",
    "keys = []\n",
    "values = []\n",
    "value_ends=[]\n",
    "\n",
    "# Iterate through the start indices of key-value pairs\n",
    "for key_value_start in key_value_end:\n",
    "    # Find the start index of the value by searching for '[[' after the key-value pair\n",
    "    value_start = dict_str.find('[[', key_value_start) + 1\n",
    "    \n",
    "    # Find the end index of the value by searching for ']]' after the value_start\n",
    "    value_end = dict_str.find(']]', value_start)+ 2\n",
    "    value_ends.append(value_end)\n",
    "\n",
    "    # Extract the value using the calculated start and end indices\n",
    "    value = dict_str[value_start:value_end]\n",
    "    values.append(value)\n",
    "    # Extract the key using the portion of the string before the '\":'\n",
    "    if key_value_start == key_value_end[0]:\n",
    "        key = dict_str[2:key_value_start].strip()\n",
    "        keys.append(key)\n",
    "    else:\n",
    "        key = dict_str[value_ends[-2]+ 3:key_value_start].strip()\n",
    "        keys.append(key)\n",
    "        \n",
    "#print(values)\n",
    "#print(value_ends)\n",
    "modified_values = {}  # Initialize an empty dictionary to store modified values\n",
    "\n",
    "# Iterate through the keys and values\n",
    "for key, value in zip(keys, values):\n",
    "    modified_value = value.replace(\" \", \"\").replace(\"],\", \"]\").replace(\"]]\",\"]\")\n",
    "    modified_values[key] = modified_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_write = [\"Rear\", \"Investigate Type 1\", \"Investigate Type 2\",\n",
    "                 \"Contra-Itch\", \"Investigate Type 3\", \"Contra-Body Groom\",\n",
    "                 \"Face Groom Type 1\", \"Dive/Scrunch\", \"Head Groom\", \"Ipsi-Orient\",\n",
    "                 \"Face Groom Type 2\", \"Ipsi-Body Groom\", \"Ipsi-Itch Type 1\",\n",
    "                 \"Paw-Groom\", \"Locomotion\", \"Contra-Orient\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to hold cleaned and possibly split values\n",
    "cleaned_values = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the keys and values, cleaning and splitting if needed\n",
    "for key in keys_to_write:\n",
    "    value = modified_values[key].strip()\n",
    "    \n",
    "    if len(value) > 32500:\n",
    "        # Find the last \"]\" before 32500 characters\n",
    "        split_index = value.rfind(\"]\", 0, 32500)\n",
    "        \n",
    "        if split_index != -1:\n",
    "            # Split the value into two parts\n",
    "            part1 = value[:split_index + 1]  # Include the last \"]\"\n",
    "            part2 = value[split_index + 1:]\n",
    "            \n",
    "            cleaned_values[key] = part1  # Store the first part\n",
    "            \n",
    "            continue_key = f\"{key} Continue\"\n",
    "            continue_count = 1\n",
    "            \n",
    "            while len(part2) > 32500:\n",
    "                continue_count += 1\n",
    "                split_index = part2.rfind(\"]\", 0, 32500)  # Find the last \"]\" in the remaining portion\n",
    "                part1 = part2[:split_index + 1]\n",
    "                part2 = part2[split_index + 1:]\n",
    "                cleaned_values[f\"{key} Continue {continue_count}\"] = part1\n",
    "            cleaned_values[f\"{key} Continue {continue_count + 1}\"] = part2  # Store the remaining part\n",
    "            \n",
    "        else:\n",
    "            # If \"]\" is not found, just store the value as it is\n",
    "            cleaned_values[key] = value\n",
    "    else:\n",
    "        cleaned_values[key] = value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format specific values in cleaned_values dictionary\n",
    "formatted_values = {'\"{}\"'.format(key): value for key, value in cleaned_values.items()}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Print the keys and lengths of values in cleaned_values dictionary\n",
    "for key, value in cleaned_values.items():\n",
    "    print(key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the list of behavior keys you want to include in mouse_data\n",
    "behavior_keys = list(cleaned_values.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(behavior_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the output path for the CSV file\n",
    "output_path = r\"Z:\\KayCei\\Practice 16.csv\" #if on Yttri-Lab\n",
    "output_path=r\"Users/Shared/K/KayCei/Practice 16.csv\"\n",
    "\n",
    "data_dict = {}\n",
    "session_numbers = {}\n",
    "\n",
    "# Iterate through each file in infiles\n",
    "for file_path in infiles: # if taking from Z Drive \n",
    "    folders = (os.path.dirname(file_path)).split('/')\n",
    "    print(folders)\n",
    "    # Extract the mouse name from the file path\n",
    "    mouse_name = fol3[0]\n",
    "    # Extract the genotype from the file path\n",
    "    genotype = fol3[1]\n",
    "    # Extract the date from the file path\n",
    "    date = fol4[0]\n",
    "    environment= fol4[1]\n",
    "#for file_path in infiles:   #if taking files from D drive\n",
    "    #folders = (os.path.dirname(file_path)).split('/')\n",
    "    #print(folders)\n",
    "    # Extract the mouse name from the file path\n",
    "    #mouse_name = folders[6].split('-')[0]\n",
    "    # Extract the genotype from the file path\n",
    "    #genotype = folders[6].split('-')[1]\n",
    "    # Extract the date from the file path\n",
    "    #date = folders[-1].split('-')[0]    \n",
    "# Determine the session number for this mouse and stim behavior\n",
    "    key = (mouse_name, environment)\n",
    "    if key not in session_numbers:\n",
    "        session_numbers[key] = 1\n",
    "    else:\n",
    "        session_numbers[key] += 1\n",
    "    session_number = session_numbers[key]\n",
    "\n",
    "   # Initialize the mouse_data dictionary\n",
    "    mouse_data = {\n",
    "        \"Mouse\": mouse_name,\n",
    "        \"Genotype\": genotype,\n",
    "        \"Date\": date,\n",
    "        \"Environment\": environment,\n",
    "        \"Session Number\": session_number,\n",
    "    }\n",
    "\n",
    "    for behavior_key in behavior_keys:\n",
    "        cleaned_value = cleaned_values.get(behavior_key, 0)\n",
    "        mouse_data[behavior_key] = cleaned_value\n",
    "\n",
    "        if not behavior_key.endswith(\"Continue\"):\n",
    "            bout_count = bout_by_key.get(behavior_key, 0)\n",
    "            average_duration = average_duration_by_key.get(behavior_key, 0)\n",
    "\n",
    "            mouse_data[f\"{behavior_key} Total Bout Count\"] = bout_count\n",
    "            mouse_data[f\"{behavior_key} Average Duration(s)\"] = average_duration\n",
    "\n",
    "            # Check if the behavior_key ends with \"Continue\"\n",
    "            if behavior_key.endswith(\"Continue\"):\n",
    "                # Remove the \"Total bout\" and \"Average duration\" columns\n",
    "                del mouse_data[f\"{behavior_key} Total Bout Count\"]\n",
    "                del mouse_data[f\"{behavior_key} Average Duration(s)\"]\n",
    "\n",
    "    # Add mouse_data to data_dict\n",
    "    data_dict[mouse_name] = mouse_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define a pattern to match \"Continue\" followed by one or more digits\n",
    "pattern = r\"Continue \\d+ (Total Bout Count|Average Duration\\(s\\))\"\n",
    "\n",
    "# Create a list of keys to remove based on the pattern\n",
    "keys_to_remove = [key for key in mouse_data.keys() if re.search(pattern, key)]\n",
    "\n",
    "# Remove the keys and their associated values from the dictionary\n",
    "for key in keys_to_remove:\n",
    "    del mouse_data[key]\n",
    "\n",
    "# Print the updated dictionary\n",
    "#print(mouse_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(keys_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a list comprehension\n",
    "#keys = [key for key in mouse_data.keys()]\n",
    "#print(keys)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Iterate through the data_dict and print mouse_data for each mouse\n",
    "for mouse_name, mouse_data in data_dict.items():\n",
    "    print(f\"Mouse Name: {mouse_name}\")\n",
    "    for key, value in mouse_data.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the headers for the CSV file\n",
    "headers = list(mouse_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(output_path):\n",
    "    # Keep track of existing entries in the CSV to avoid duplicates\n",
    "    existing_entries = set()\n",
    "    entry_key2_session_numbers = {}\n",
    "    sorted_entries = []\n",
    "\n",
    "    # Read existing data from the CSV and add it to the set\n",
    "    with open(output_path, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile, delimiter=';')\n",
    "        for row in reader:\n",
    "            entry_key = (row[\"Mouse\"], row[\"Date\"], row[\"Environment\"])\n",
    "            entry_key2 = (row[\"Mouse\"], row[\"Date\"], row[\"Environment\"], row[\"Genotype\"])\n",
    "            existing_entries.add(entry_key)\n",
    "            entry_key2_session_numbers[entry_key2] = int(row[\"Session Number\"])\n",
    "            sorted_entries.append(row)\n",
    "        \n",
    "else:\n",
    "    existing_entries = set()\n",
    "    entry_key2_session_numbers = {}\n",
    "    \n",
    "# Ensure the output directory exists before writing the CSV file\n",
    "output_dir = os.path.dirname(output_path)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Write the data from data_dict to the CSV file (in append mode 'a')\n",
    "with open(output_path, 'a', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=headers, delimiter=';')\n",
    "\n",
    "    # Check if the file is empty, if so, write the header\n",
    "    if csvfile.tell() == 0:\n",
    "        writer.writeheader()\n",
    "\n",
    "for mouse_data in data_dict.values():\n",
    "    entry_key = (mouse_data[\"Mouse\"], mouse_data[\"Date\"], mouse_data[\"Environment\"])\n",
    "    entry_key2 = (mouse_data[\"Mouse\"], mouse_data[\"Date\"], mouse_data[\"Environment\"])\n",
    "\n",
    "    # Check if the entry already exists in the CSV\n",
    "    if entry_key not in existing_entries:\n",
    "        session_number = entry_key2_session_numbers.get(entry_key2, 0) + 1\n",
    "        existing_entries.add(entry_key)\n",
    "        entry_key2_session_numbers[entry_key2] = session_number\n",
    "\n",
    "        for formatted_key in behavior_keys:\n",
    "            if formatted_key in formatted_values:\n",
    "                mouse_data[formatted_key] = formatted_values[formatted_key]\n",
    "\n",
    "        # Write the complete mouse data to the CSV\n",
    "        sorted_entries.append(mouse_data)  # Add the new entry\n",
    "\n",
    "        print(\"Entry added Successfully:\", entry_key)\n",
    "    else:\n",
    "        print(\"Entry already exists in the CSV:\", entry_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add unique keys from unique_entry_keys to fieldnames\n",
    "fieldnames=headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the fieldnames list to a set\n",
    "fieldnames_set = set(headers)  # Replace 'headers' with your actual fieldnames list\n",
    "\n",
    "# Extract the keys from the dictionaries within sorted_entries and create a set\n",
    "entry_keys_set = set()\n",
    "for entry in sorted_entries:\n",
    "    entry_keys_set.update(entry.keys())\n",
    "\n",
    "# Find the unique keys in fieldnames_set and entry_keys_set\n",
    "unique_fieldnames = fieldnames_set.difference(entry_keys_set)\n",
    "unique_entry_keys = entry_keys_set.difference(fieldnames_set)\n",
    "\n",
    "# Print the unique keys\n",
    "print(\"Unique keys in fieldnames_set:\", unique_fieldnames)\n",
    "print(\"Unique keys in entry_keys_set:\", unique_entry_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for unique_entry_key in unique_entry_keys:\n",
    "    # Check if unique_entry_key exists (not None and not an empty string)\n",
    "    if unique_entry_key:\n",
    "        # Extract the prefix by splitting at the first space followed by a digit\n",
    "        prefix_to_match = re.split(r'\\s+\\d', unique_entry_key)[0]\n",
    "\n",
    "        # Find the position to insert unique_entry_key\n",
    "        insert_position = None\n",
    "\n",
    "        for i in range(len(fieldnames) - 1, -1, -1):\n",
    "            if fieldnames[i].startswith(prefix_to_match):\n",
    "                insert_position = i + 1\n",
    "                break\n",
    "\n",
    "        # If the prefix is not found, append the key to the end\n",
    "        if insert_position is None:\n",
    "            insert_position = len(fieldnames)\n",
    "\n",
    "        # Insert the unique_entry_key at the specified position\n",
    "        fieldnames.insert(insert_position, unique_entry_key)\n",
    "    else:\n",
    "        # Skip to the next iteration (next unique_entry_key)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sort the entries based on date within each group of entries that share the same entry_key2\n",
    "sorted_entries.sort(key=lambda x: (x[\"Mouse\"], x[\"Date\"], x[\"Environment\"]))\n",
    "\n",
    "# Update the session numbers based on the sorted order\n",
    "entry_key2_counter = {}  # To track the session numbers within each entry_key2 group\n",
    "\n",
    "for entry in sorted_entries:\n",
    "    entry_key2 = (entry[\"Mouse\"], entry[\"Date\"], entry[\"Environment\"], entry[\"Genotype\"])\n",
    "    \n",
    "    if entry_key2 not in entry_key2_counter:\n",
    "        entry_key2_counter[entry_key2] = 1\n",
    "    else:\n",
    "        entry_key2_counter[entry_key2] += 1\n",
    "    \n",
    "    entry[\"Session Number\"] = entry_key2_counter[entry_key2]\n",
    "\n",
    "# Sort the entries based on date within each group of entries that share the same entry_key2\n",
    "sorted_entries.sort(key=lambda x: (x[\"Mouse\"], x[\"Date\"], x[\"Environment\"], x[\"Genotype\"]))\n",
    "\n",
    "# Rewrite the CSV file with sorted and formatted entries, including any missing fields\n",
    "with open(output_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter=';')\n",
    "    writer.writeheader()\n",
    "    writer.writerows(sorted_entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
